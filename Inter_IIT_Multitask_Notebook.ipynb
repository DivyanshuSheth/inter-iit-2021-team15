{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TTjh7TUuQWQO",
        "S5m7__FNeQxo",
        "pwX1sZQMd3AJ",
        "Ui2rqtPHe6Oj"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "21ce3e7658b54e39ac3fd5d36d4988ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_25e6572dac304d25904e1cdd9f1c24ab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cc86f275dff54b9ca4a990f3f7cf6aab",
              "IPY_MODEL_45b279f00c4a41d3bd4211298a04880d"
            ]
          }
        },
        "25e6572dac304d25904e1cdd9f1c24ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc86f275dff54b9ca4a990f3f7cf6aab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_29ee818a97d241d1bf4506561fbb7677",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 512,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 512,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1e8373d5c3e4bc0bb5ae7c9ee8942c4"
          }
        },
        "45b279f00c4a41d3bd4211298a04880d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aa16ef3750a3494282bd357f1f630975",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 512/512 [00:00&lt;00:00, 906B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8aa2e2c70bca44e1b132e7452e9174a6"
          }
        },
        "29ee818a97d241d1bf4506561fbb7677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1e8373d5c3e4bc0bb5ae7c9ee8942c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa16ef3750a3494282bd357f1f630975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8aa2e2c70bca44e1b132e7452e9174a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "51a013cac54e44c4b464cf030feec4fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fd9132ec9b924bbd82946cb70b721f4d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c62b4f15067045e880509e24571d43b2",
              "IPY_MODEL_8410f01fa86240caba0980743e2b8e05"
            ]
          }
        },
        "fd9132ec9b924bbd82946cb70b721f4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c62b4f15067045e880509e24571d43b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_26cd7b1e3e04477982f7fc0657a62ab9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5069051,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5069051,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40233e3a22eb48d98f48bbd2118a0dc6"
          }
        },
        "8410f01fa86240caba0980743e2b8e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_00b567cd8b94449eb5629f9dfcd94197",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.07M/5.07M [00:30&lt;00:00, 169kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aec63575b01e4a2c91ca7e3a8b51a144"
          }
        },
        "26cd7b1e3e04477982f7fc0657a62ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40233e3a22eb48d98f48bbd2118a0dc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00b567cd8b94449eb5629f9dfcd94197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aec63575b01e4a2c91ca7e3a8b51a144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "43c1751201dd46efbf5ebf7d912d3617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_630e43e743334a00881def84e4f39f3e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d69bac052879463dbb8e200e5c2cdbcb",
              "IPY_MODEL_ae21ea59760240618a715a50a48bf43f"
            ]
          }
        },
        "630e43e743334a00881def84e4f39f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d69bac052879463dbb8e200e5c2cdbcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b9aaf5913e0d4e37b22166fc15feb6bc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9096718,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9096718,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_112e667d0805470e878c912abab7f972"
          }
        },
        "ae21ea59760240618a715a50a48bf43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8fe8aced2d174df8ae938959320eb6e1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9.10M/9.10M [00:01&lt;00:00, 4.59MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0105488cfb44408aa66396e2baea574d"
          }
        },
        "b9aaf5913e0d4e37b22166fc15feb6bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "112e667d0805470e878c912abab7f972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8fe8aced2d174df8ae938959320eb6e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0105488cfb44408aa66396e2baea574d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a87d9626bfc451395181ff4be231622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_26a2d72dc3d4440aa9d05f9358249158",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4d4e6e916f7b4d7fbcd64d1a772ea08a",
              "IPY_MODEL_9cb52343a24749a1977774f46806fae2"
            ]
          }
        },
        "26a2d72dc3d4440aa9d05f9358249158": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d4e6e916f7b4d7fbcd64d1a772ea08a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f65d4b1cac98448292172943b1effeeb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1115590446,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1115590446,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f788116ceb2a47468f6cb49ad92e6b8f"
          }
        },
        "9cb52343a24749a1977774f46806fae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_56857d0d383b4fa9862f5b1ce3d17b28",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.12G/1.12G [00:21&lt;00:00, 52.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f5acc16fe8ee46d690a604fd2013c2a4"
          }
        },
        "f65d4b1cac98448292172943b1effeeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f788116ceb2a47468f6cb49ad92e6b8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56857d0d383b4fa9862f5b1ce3d17b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f5acc16fe8ee46d690a604fd2013c2a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVH_Lsqoul3b"
      },
      "source": [
        "# Inter-IIT Team:15\n",
        "* Downloading/Reading Files -> Train and/or Test, utility\n",
        "* Preprocessing\n",
        "* Classification\n",
        "* Entity Extraction\n",
        "* Sentiment Analysis\n",
        "* Headline generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qElGOZtLp_v"
      },
      "source": [
        "##Inter-IIT Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufA8q2YFbIdP"
      },
      "source": [
        "### Download and Reading Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3LrpDP7CxFU"
      },
      "source": [
        "#### Raw and preprocessed versions of Inter-IIT train and dev files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doiLm42WyfVf"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAgUoI40BlYI",
        "outputId": "3b38606a-19e0-4d46-cb77-9e4c9be875f0"
      },
      "source": [
        "! rm -r sample_data\n",
        "\n",
        "test = True\n",
        "%cd \"/content\"\n",
        "\n",
        "if not test:\n",
        "    ############ Original Training files for Articles and Tweets - With headline, text id, label and text/tweet\n",
        "    ! mkdir \"Development_Data\"\n",
        "\n",
        "    %cd \"/content/Development_Data/\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/qnrv8gri4erad60/dev_data_article.xlsx?dl=1\" -O \"dev_data_article.xlsx\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/r9idyef7g9kzyi0/dev_data_tweet.xlsx?dl=1\" -O \"dev_data_tweet.xlsx\"\n",
        "\n",
        "    ! mkdir \"Tweets\" \"Articles\" \"Tweets/Raw\" \"Tweets/Preprocessed_NoTransliteration\" \"Tweets/Preprocessed_Transliterated\" \"Tweets/Transliterated_Nouns\" \"Articles/Raw\" \"Articles/Preprocessed_NoTransliteration\" \"Articles/Preprocessed_Transliterated\" \"Articles/Transliterated_Nouns\"\n",
        "    \n",
        "\n",
        "    ############ Raw Training Files for Tweets - With only tweet and label\n",
        "    %cd \"/content/Development_Data/Tweets/Raw/\" \n",
        "    ! wget -q \"https://www.dropbox.com/scl/fi/dkr2fhefcb31ucgrw0grc/tweets_train_raw-1.xlsx?dl=0&rlkey=vkuuzqu3wasy93rkjfwtx41wu\" -O \"tweets_train_raw.xlsx\"\n",
        "    ! wget -q \"https://www.dropbox.com/scl/fi/hsca5e5m9knkkirdxdm4x/tweets_test_raw-1.xlsx?dl=0&rlkey=br0f2lzj4qfnyvd7exw0cs1qa\" -O \"tweets_test_raw.xlsx\"\n",
        "    ############ Preprocessed Training Files for Tweets - Transliterated and non transliterated\n",
        "    %cd \"/content/Development_Data/Tweets/Preprocessed_Transliterated/\"\n",
        "    ! wget -q \"https://www.dropbox.com/scl/fi/y1udlzfimkvng2byzlp94/tweets_test_preprocessed_Transliterated-2.xlsx?dl=0&rlkey=48lsyiwzkl1k6t32bv4xxs3pq\" -O \"tweets_test_preprocessed_Transliterated.xlsx\"\n",
        "    ! wget -q \"https://www.dropbox.com/scl/fi/mjxkjjnelbvon6jly5z5x/tweets_train_preprocessed_Transliterated-2.xlsx?dl=0&rlkey=fst839rvop5ifgpjbitl5oo4o\" -O \"tweets_train_preprocessed_Transliterated.xlsx\" \n",
        "    %cd \"/content/Development_Data/Tweets/Preprocessed_NoTransliteration\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/m62gh9wfdsyx51m/tweets_test_preprocessed_NOTransliteration.xlsx?dl=1\" -O \"tweets_test_preprocessed_NOTransliteration.xlsx\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/85edilzlyd9uefn/tweets_train_preprocessed_NOTransliteration.xlsx?dl=1\" -O \"tweets_train_preprocessed_NOTransliteration.xlsx\"\n",
        "    ############ Tweets preprocessed with extracted nouns\n",
        "    %cd \"/content/Development_Data/Tweets/Transliterated_Nouns\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/xllrsn9xrcwjxt4/space_train_tweets_spacyPOS.xlsx?dl=1\" -O \"tweets_test_transliterated_nouns.xlsx\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/xllrsn9xrcwjxt4/space_train_tweets_spacyPOS.xlsx?dl=1\" -O \"tweets_train_transliterated_nouns.xlsx\"\n",
        "\n",
        "    ############ Raw Training Files for Articles - With only text and label\n",
        "    %cd \"/content/Development_Data/Articles/Raw\"\n",
        "    ! wget -q \"https://www.dropbox.com/scl/fi/wgnojlhj98sqo00lbl4jg/articles_train_raw.xlsx?dl=0&rlkey=dhblh38juxqi9g9b9dmfb2ndh\" -O \"articles_train_raw.xlsx\"\n",
        "    ! wget -q \"https://www.dropbox.com/scl/fi/y7w9lkwph8620udglu37l/articles_test_raw.xlsx?dl=0&rlkey=rifvk5vknywp5679pwjogcyuu\" -O \"articles_test_raw.xlsx\"\n",
        "    ###########Preprocessed Training Files for Articles - Transliterated and non transliterated\n",
        "    %cd \"/content/Development_Data/Articles/Preprocessed_Transliterated/\"\n",
        "    ! wget -q \"https://www.dropbox.com/scl/fi/3uo4kv5g4zybj5w8fgu1y/articles_test_preprocessed_Transliterated-2.xlsx?dl=0&rlkey=szvdb2j572bn2g4idcd89qoem\" -O \"articles_test_preprocessed_Transliterated.xlsx\"\n",
        "    ! wget -q \"https://www.dropbox.com/scl/fi/2puppr1yrluklwta5io34/articles_train_preprocessed_Transliterated-2.xlsx?dl=0&rlkey=en0nebw1zlbn8c6kc71yd0uhl\" -O \"articles_train_preprocessed_Transliterated.xlsx\"\n",
        "    %cd \"/content/Development_Data/Articles/Preprocessed_NoTransliteration\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/xb14qx5z015shjk/articles_test_preprocessed_NOTransliteration.xlsx?dl=1\" -O \"articles_test_preprocessed_NOTransliteration.xlsx\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/ni33m0rz3o07zo9/articles_train_preprocessed_NOTransliteration.xlsx?dl=1\" -O \"articles_train_preprocessed_NOTransliteration.xlsx\"\n",
        "    ############ Articles preprocessed with extracted nouns\n",
        "    %cd \"/content/Development_Data/Articles/Transliterated_Nouns\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/w17x62baq7hgvzy/space_test_articles_spacyPOS.xlsx?dl=1\" -O \"articles_test_transliterated_nouns.xlsx\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/u63xbhlw9xhjxs7/space_train_articles_spacyPOS.xlsx?dl=1\" -O \"articles_train_transliterated_nouns.xlsx\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5EUYsBHbmsR"
      },
      "source": [
        "#### Raw and preprocessed version of Inter-IIT test files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwXrtuKgbm7m",
        "outputId": "995ea4ce-ba61-4c76-ab3b-312d4f7fc6e1"
      },
      "source": [
        "test = True\n",
        "%cd \"/content\"\n",
        "\n",
        "if test:\n",
        "    ############ Original Training files for Articles and Tweets - With headline, text id, label and text/tweet\n",
        "    ! mkdir \"Eval_Data\"\n",
        "\n",
        "    %cd \"/content/Eval_Data/\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/k7f4fatqe4mo7ew/evaluation_data.xlsx?dl=1\" -O \"evaluation_raw.xlsx\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/0622nq0xm11abcc/eval_processed_nouns.xlsx?dl=1\" -O \"evaluation_processed_nouns.xlsx\"\n",
        "\n",
        "    ! mkdir \"Tweets\" \"Articles\" \"Tweets/Raw\" \"Tweets/Transliterated_Nouns\" \"Articles/Raw\" \"Articles/Transliterated_Nouns\"\n",
        "    \n",
        "\n",
        "    ############ Raw Eval Files for Tweets\n",
        "    %cd \"/content/Eval_Data/Tweets/Raw/\" \n",
        "    ! wget -q \"https://www.dropbox.com/s/wv3p2lrznxkw66v/tweets_raw.xlsx?dl=1\" -O \"tweets_eval_raw.xlsx\"\n",
        "    ############ Tweets preprocessed with extracted nouns\n",
        "    %cd \"/content/Eval_Data/Tweets/Transliterated_Nouns\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/07grgpqcd6js4up/tweets_processed%20%281%29.xlsx?dl=0\" -O \"tweets_eval_processed.xlsx\"\n",
        "\n",
        "    ############ Raw Eval Files for Articles\n",
        "    %cd \"/content/Eval_Data/Articles/Raw\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/zf4w3qgaul0vt0y/articles_raw.xlsx?dl=1\" -O \"articles_eval_raw.xlsx\"\n",
        "    ############ Articles preprocessed with extracted nouns\n",
        "    %cd \"/content/Eval_Data/Articles/Transliterated_Nouns\"\n",
        "    ! wget -q \"https://www.dropbox.com/s/vwi71sksd84o1lu/articles_processed%20%281%29.xlsx?dl=0\" -O \"articles_eval_processed.xlsx\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/Eval_Data\n",
            "/content/Eval_Data/Tweets/Raw\n",
            "/content/Eval_Data/Tweets/Transliterated_Nouns\n",
            "/content/Eval_Data/Articles/Raw\n",
            "/content/Eval_Data/Articles/Transliterated_Nouns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II6pHW5DZAed"
      },
      "source": [
        "#### Scraped Articles and Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVPUdBnNY_72"
      },
      "source": [
        "test = True\n",
        "if not test:\n",
        "    %cd \"/content/\"\n",
        "    ! mkdir \"ScrapedData\"\n",
        "    %cd \"/content/ScrapedData\"\n",
        "\n",
        "    # Scraped data articles:\n",
        "    ! wget -q \"https://www.dropbox.com/s/szcamsk6m3aka7g/scrapped_articles_preprocessed.csv?dl=1\" -O \"scraped_articles_preprocessed.csv\"\n",
        "\n",
        "    # Scraped data tweets:\n",
        "    ! wget -q \"https://www.dropbox.com/s/nasdzwu4otc3m2u/scraped_tweets_preprocessed.csv?dl=0\" -O \"scraped_tweets_preprocessed.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm521XL5C9eu"
      },
      "source": [
        "#### Additional Utility Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjIZ3v7kDKtO",
        "outputId": "af0f114c-8f4c-4e06-c333-9aa4c7c7cddd"
      },
      "source": [
        "########## Additional utility files -> Hinglish->Devnagri mapping and Hindi Stopwords\n",
        "%cd \"/content/\"\n",
        "! wget -q \"https://www.dropbox.com/s/xnopa8bjw9t24zj/stopwords_hindi.txt?dl=1\" -O \"stopwords_hindi.txt\"\n",
        "! wget -q \"https://www.dropbox.com/s/r3k95b6vwolhu1d/final_transliteration%20%281%29.txt?dl=1\" -O \"transliteration_final.txt\"\n",
        "! wget -q \"https://www.dropbox.com/s/h9uj8r8i3n9yvz5/hindibrands-englishbrands.txt?dl=1\" -O \"hindi-english_brands.txt\"\n",
        "\n",
        "!wget -q \"https://www.dropbox.com/s/9gdwt9t0ltoh5je/eng_brands.csv?dl=1\" -O \"eng_brands.csv\"\n",
        "!wget -q \"https://www.dropbox.com/s/ol60op5u66qji3g/eng.json?dl=1\" -O \"eng.json\"\n",
        "!wget -q \"https://www.dropbox.com/s/3pivi81q14du800/Hindi-brands%20%283%29.xlsx?dl=1\" -O \"Hindi-brands.xlsx\"\n",
        "!wget -q \"https://www.dropbox.com/s/c5s5h1kzse1moj9/hbrands2%20%281%29.json?dl=1\" -O \"hbrands2.json\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk_WDfTr8Z6_"
      },
      "source": [
        "###Libraries for Inter-IIT Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX_xPe5OO1WP",
        "outputId": "50fc63cb-0fbf-4734-f9ec-b379303342f0"
      },
      "source": [
        "!pip install emoji\n",
        "!pip install ekphrasis\n",
        "!pip install spacy_langdetect\n",
        "!pip install fuzzywuzzy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 22.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 30kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 61kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 71kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 81kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 92kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 102kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 112kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 122kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 7.8MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n",
            "Collecting ekphrasis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.41.1)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 14.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Building wheels for collected packages: ekphrasis, ftfy\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp37-none-any.whl size=82844 sha256=62d09954767774b38179652778a77ba99845a3fd99c3101be8c739a34726152d\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp37-none-any.whl size=46451 sha256=4fdb5d735977cb1869af784bef5a5d54c3e107a20d9654d33c83ff536ba44c9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "Successfully built ekphrasis ftfy\n",
            "Installing collected packages: colorama, ujson, ftfy, ekphrasis\n",
            "Successfully installed colorama-0.4.4 ekphrasis-0.5.1 ftfy-5.9 ujson-4.0.2\n",
            "Collecting spacy_langdetect\n",
            "  Downloading https://files.pythonhosted.org/packages/29/70/72dad19abe81ca8e85ff951da170915211d42d705a001d7e353af349a704/spacy_langdetect-0.1.2-py3-none-any.whl\n",
            "Collecting langdetect==1.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from spacy_langdetect) (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect==1.0.7->spacy_langdetect) (1.15.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (8.7.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (20.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (54.1.2)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (0.7.1)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-cp37-none-any.whl size=993460 sha256=f257ae478dfe76e9d435b6f85072e448c632d3a984670dd912ec04932a0299a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect, spacy-langdetect\n",
            "Successfully installed langdetect-1.0.7 spacy-langdetect-0.1.2\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYoj0be3b-v1"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RifN8tJkLWjw"
      },
      "source": [
        "#### Create a dictionary mapping of hindi words written in roman script to devnagari to support transliteration, and a dictionary of devanagari brands to english brands to support brand identification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3km3PSdaLUqQ",
        "outputId": "a7ea740b-64d4-423e-8322-640ada7a2159"
      },
      "source": [
        "import ast\n",
        "\n",
        "f = open('/content/transliteration_final.txt', 'r')\n",
        "transliterate_dict = {}\n",
        "lines = f.readlines()\n",
        "for line in lines:\n",
        "    key = line.split()[0]\n",
        "    value = ' '.join(line.split()[1:])\n",
        "    transliterate_dict[key] = value.rstrip('\\n')\n",
        "print(\"Number of transliterated pairs : \",len(transliterate_dict))\n",
        "f.close()\n",
        "\n",
        "f = open(\"/content/hindi-english_brands.txt\", \"r\")\n",
        "contents = f.read()\n",
        "devanagari_brands = ast.literal_eval(contents)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of transliterated pairs :  23694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiNh1e8SO7O0"
      },
      "source": [
        "####Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHjxyBWqO7m1"
      },
      "source": [
        "def print_df(df):\n",
        "    print(\"Shape of dataset : {}\".format(df.shape))\n",
        "    print(\"Class Distribution : \")\n",
        "    print(\"\\n\")\n",
        "    print(df.sample(5))\n",
        "\n",
        "def drop_redundant(df, minArticleLength = 50):\n",
        "    storing_indices = df[df['text'].map(len) < minArticleLength].index #to be later added to the test data\n",
        "    df.drop(df[df['text'].map(len) < minArticleLength].index, inplace = True)\n",
        "    return df\n",
        "\n",
        "def POSextraction(df):\n",
        "    '''\n",
        "    Returns a df with an added column 'nouns' where for each input text, corresponding nouns (NOUN AND PROPN) are extracted\n",
        "    Input: DataFrame\n",
        "    Output: Updated DataFrame, with an added column 'nouns' where nouns are extracted for each input text, and separated by a space (' ')\n",
        "    '''\n",
        "    nouns_list = []\n",
        "    for i in range(len(df)):\n",
        "        temp_nouns = \"\"\n",
        "        doc = nlp(df.at[i, 'text'])\n",
        "        for token in doc:\n",
        "            if token.pos_ == \"PROPN\" or token.pos_ == \"NOUN\":\n",
        "                temp_nouns += token.text + \" \"\n",
        "        nouns_list.append(temp_nouns)\n",
        "    df = df.assign(nouns = nouns_list)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC422u5MDKEy"
      },
      "source": [
        "#### Preprocessing class for Inter-IIT Tweet Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvDmaA50CwSd",
        "outputId": "c05688e5-a6ff-4f78-c3a9-b607e6ef5bb4"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import emoji\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "import spacy\n",
        "from spacy_langdetect import LanguageDetector\n",
        "# from google_trans_new import google_translator\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "seg = Segmenter()\n",
        "nlp = spacy.load('en')  # 1\n",
        "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
        "\n",
        "class InterIITTweetPreprocessing:\n",
        "    def __init__(self, run_transliteration=True):\n",
        "        self.run_transliteration = run_transliteration # flag for running transliteration; default value True\n",
        "\n",
        "    def run(self, raw_str):\n",
        "        if self.run_transliteration:\n",
        "            preprocessed_str = self.transliterate(self.segmenter(self.replace_hindi_brands(self.remove_punctuation(self.demojify(self.remove_URL(self.remove_htmltags(self.remove_users(raw_str))))))).lower())\n",
        "        else:\n",
        "            preprocessed_str = self.segmenter(self.replace_hindi_brands(self.remove_punctuation(self.demojify(self.remove_URL(self.remove_htmltags(self.remove_users(raw_str))))))).lower()\n",
        "        return preprocessed_str\n",
        "        \n",
        "    def only_nonascii(self, input_str):\n",
        "        \"\"\"Substitutes ASCII characters with empty string\"\"\"\n",
        "        _ascii_letters = re.compile(r'[a-zA-Z0-9/.:-]', flags=re.UNICODE)\n",
        "        return _ascii_letters.sub(\"\", input_str)\n",
        "\n",
        "    def remove_URL(self, input_str):\n",
        "        \"\"\"Remove URLs from an input string\"\"\"\n",
        "        return re.sub(r\"http\\S+\", \"\", input_str)\n",
        "        \n",
        "    def remove_users(self, input_str):\n",
        "        \"\"\"Removes strings @userhandle to clean twitter data\"\"\"\n",
        "        raw_str = re.sub(r\"@\\w+\", '', input_str).strip() # .lstrip('RT').lstrip('QT')\n",
        "        final_str = re.sub('RT|QT' , '', str(raw_str))\n",
        "        return final_str\n",
        "\n",
        "    def remove_htmltags(self, input_str):\n",
        "        \"\"\"Removes html tags of the form <any character(s)> from an input string\"\"\"\n",
        "        return re.sub(r\"<.*?>\", \"\", input_str)\n",
        "\n",
        "    def remove_non_ascii(self, input_str):\n",
        "        \"\"\"Remove non-ASCII characters from an input string\"\"\"\n",
        "        tokenized_words = word_tokenize(input_str)\n",
        "        tokens = []\n",
        "        for token in tokenized_words:\n",
        "            cleaned_token = unicodedata.normalize('NFKD', token).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "            tokens.append(cleaned_token)\n",
        "        return ' '.join(tokens)\n",
        "    \n",
        "    def remove_punctuation(self, sample_str):\n",
        "        \"\"\"Remove punctuation from a input string\"\"\"\n",
        "        comment = sample_str\n",
        "        comment = re.sub(\n",
        "        r\"[\\*\\\"“”\\|\\~\\n\\\\\\@\\{\\}\\%\\_\\#\\^\\&\\$\\£\\+\\<\\>\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
        "        str(comment))\n",
        "        comment = re.sub(r\"[ ]+\", \" \", comment)\n",
        "        comment = re.sub(r\"\\!+\", \"!\", comment)\n",
        "        comment = re.sub(r\"\\,+\", \",\", comment)\n",
        "        comment = re.sub(r\"\\?+\", \"?\", comment)\n",
        "        return comment\n",
        "\n",
        "    def segmenter(self, input_str):\n",
        "        \"\"\"Performs segmentation using the ekphasis library, it splits 2-3 joined words into proper words\"\"\"\n",
        "        segmented_string = \"\"\n",
        "        for raw_token in input_str.split():\n",
        "           segmented_string += \" \"  +  seg.segment(raw_token)\n",
        "        return segmented_string\n",
        "\n",
        "    def replace_nan(self, input_str):\n",
        "        \"\"\"Replacing nan strings with empty strings - required for textrank\"\"\"        \n",
        "        input_str = re.sub('nan' , '' , str(input_str))\n",
        "        return input_str\n",
        "    \n",
        "    def demojify(self, input_str):\n",
        "        \"\"\"Convert emojis to text - and remove the colon(:)\"\"\"\n",
        "        input_str = emoji.demojize(input_str)\n",
        "        input_str = re.sub(':|_' , ' ' , str(input_str))\n",
        "        # input_str = re.sub('_' , ' ' , str(input_str))\n",
        "        return input_str\n",
        "\n",
        "    def transliterate(self, input_str):\n",
        "        \"\"\"Transliterate back to Devanagari script\"\"\"\n",
        "        doc = nlp(input_str) \n",
        "        detect_language = doc._.language\n",
        "        if detect_language['language'] == 'en' and detect_language['score'] > 0.8:\n",
        "            return input_str\n",
        "        raw_tokens = input_str.split()\n",
        "        transliterated_str = ''\n",
        "        for raw_token in raw_tokens:\n",
        "            try:\n",
        "                transliterated_str += transliterate_dict[raw_token] + ' '\n",
        "            except:\n",
        "                transliterated_str += raw_token + ' '\n",
        "        return transliterated_str\n",
        "    \n",
        "    def replace_hindi_brands(self, input_str):\n",
        "        \"\"\"Replace Hindi mobile brand names by their English counterparts\"\"\"\n",
        "        replaced_str = ''\n",
        "        raw_tokens = input_str.split()\n",
        "        for raw_token in raw_tokens:\n",
        "            try:\n",
        "                replaced_str += devanagari_brands[raw_token] + ' '\n",
        "            except:\n",
        "                replaced_str += raw_token + ' '\n",
        "        return replaced_str\n",
        "\n",
        "    # def translator(self, input_str):\n",
        "    #     \"\"\"Use google API to translate hindi script to english\"\"\"\n",
        "    #     return google_translator().translate(text=input_str, lang_src='hi', lang_tgt='en')\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading english - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/english/counts_1grams.txt\n",
            "Reading english - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/english/counts_2grams.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTjh7TUuQWQO"
      },
      "source": [
        "#### Preprocessing tweet data and generating the preprocessed csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTDW6JD-QVYk"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "preprocess = False\n",
        "test = True\n",
        "train_datapath_tweets = \"/content/Development_Data/Tweets/Raw/tweets_train_raw.xlsx\"\n",
        "val_datapath_tweets = \"/content/Development_Data/Tweets/Raw/tweets_test_raw.xlsx\"\n",
        "\n",
        "if not test:\n",
        "    if preprocess:\n",
        "        preprocessor = InterIITTweetPreprocessing()      ############################## MODIFY THIS PATH IF NEEDED\n",
        "\n",
        "        train_data_tweets = pd.read_excel(train_datapath_tweets, engine='openpyxl', usecols=[\"Tweet\", \"Mobile_Tech_Tag\"])\n",
        "        train_data_tweets.columns = [\"text\",\"labels\"]\n",
        "        train_data_tweets.drop_duplicates(subset = [\"text\"], inplace = True, ignore_index = True)\n",
        "        train_data_tweets[\"text\"] = train_data_tweets['text'].apply(lambda x : preprocessor.run(x))\n",
        "        train_data_tweets = POSextraction(train_data_tweets)\n",
        "        print_df(train_data_tweets)\n",
        "        train_path_tweets = '/content/Development_Data/Tweets/Transliterated_Nouns/tweets_train_transliterated_nouns.xlsx'\n",
        "\n",
        "        val_data_tweets = pd.read_excel(val_datapath_tweets, engine='openpyxl', usecols=[\"Tweet\", \"Mobile_Tech_Tag\"])\n",
        "        val_data_tweets.columns = [\"text\",\"labels\"]\n",
        "        val_data_tweets.drop_duplicates(subset = [\"text\"], inplace = True, ignore_index = True)     \n",
        "        val_data_tweets[\"text\"] = val_data_tweets['text'].apply(lambda x : preprocessor.run(x))\n",
        "        print_df(val_data_tweets)\n",
        "        val_path_tweets = '/content/Development_Data/Tweets/Transliterated_Nouns/tweets_test_transliterated_nouns.xlsx'\n",
        "\n",
        "        train_data_tweets.to_excel(train_path_tweets,index=False, encoding='UTF-8')\n",
        "        val_data_tweets.to_excel(val_path_tweets,index=False, encoding='UTF-8')\n",
        "        \n",
        "    else :\n",
        "        train_path_tweets = '/content/Development_Data/Tweets/Transliterated_Nouns/tweets_train_transliterated_nouns.xlsx'\n",
        "        val_path_tweets = '/content/Development_Data/Tweets/Transliterated_Nouns/tweets_test_transliterated_nouns.xlsx'\n",
        "        train_data_tweets = pd.read_excel(train_path_tweets, usecols=[\"text\", \"labels\", \"nouns\"])\n",
        "        val_data_tweets = pd.read_excel(val_path_tweets, usecols=[\"text\", \"labels\", \"nouns\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKvwEmK_NK2p"
      },
      "source": [
        "#### Preprocessing class for Inter-IIT Article Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T63TWddPMnX6",
        "outputId": "bd498fdf-5b41-470c-fe36-c678250a4c0e"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import emoji\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "import spacy\n",
        "from spacy_langdetect import LanguageDetector\n",
        "# from google_trans_new import google_translator\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "seg = Segmenter()\n",
        "nlp = spacy.load('en')  # 1\n",
        "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
        "\n",
        "class InterIITArticlePreprocessing:\n",
        "    def __init__(self, run_transliteration=True):\n",
        "        self.run_transliteration = run_transliteration # flag for running transliteration; default value True\n",
        "\n",
        "    def run(self, raw_str):\n",
        "        if self.run_transliteration:\n",
        "            preprocessed_str = self.transliterate(self.segmenter(self.replace_hindi_brands(self.remove_punctuation(self.remove_URL(self.remove_htmltags(raw_str))))).lower())\n",
        "        else:\n",
        "            preprocessed_str = self.segmenter(self.replace_hindi_brands(self.remove_punctuation(self.remove_URL(self.remove_htmltags(raw_str))))).lower()\n",
        "        return preprocessed_str\n",
        "\n",
        "    def only_nonascii(self, input_str):\n",
        "        \"\"\"Substitutes ASCII characters with empty string\"\"\"\n",
        "        _ascii_letters = re.compile(r'[a-zA-Z0-9/.:-]', flags=re.UNICODE)\n",
        "        return _ascii_letters.sub(\"\", input_str)\n",
        "\n",
        "    def remove_URL(self, input_str):\n",
        "        \"\"\"Remove URLs from an input string\"\"\"\n",
        "        return re.sub(r\"http\\S+\", \"\", input_str)\n",
        "        \n",
        "    def remove_users(self, input_str):\n",
        "        \"\"\"Removes strings @userhandle to clean twitter data\"\"\"\n",
        "        raw_str = re.sub(r\"@\\w+\", '', input_str).strip() # .lstrip('RT').lstrip('QT')\n",
        "        final_str = re.sub('RT|QT' , '', str(raw_str))\n",
        "        return final_str\n",
        "\n",
        "    def remove_htmltags(self, input_str):\n",
        "        \"\"\"Removes html tags of the form <any character(s)> from an input string\"\"\"\n",
        "        return re.sub(r\"<.*?>\", \"\", input_str)\n",
        "\n",
        "    def remove_non_ascii(self, input_str):\n",
        "        \"\"\"Remove non-ASCII characters from an input string\"\"\"\n",
        "        tokenized_words = word_tokenize(input_str)\n",
        "        tokens = []\n",
        "        for token in tokenized_words:\n",
        "            cleaned_token = unicodedata.normalize('NFKD', token).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "            tokens.append(cleaned_token)\n",
        "        return ' '.join(tokens)\n",
        "    \n",
        "    def remove_punctuation(self, sample_str):\n",
        "        \"\"\"Remove punctuation from a input string\"\"\"\n",
        "        comment = sample_str\n",
        "        comment = re.sub(\n",
        "        r\"[\\*\\\"“”\\|\\~\\n\\\\\\@\\{\\}\\%\\_\\#\\^\\&\\$\\£\\+\\<\\>\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
        "        str(comment))\n",
        "        comment = re.sub(r\"[ ]+\", \" \", comment)\n",
        "        comment = re.sub(r\"\\!+\", \"!\", comment)\n",
        "        comment = re.sub(r\"\\,+\", \",\", comment)\n",
        "        comment = re.sub(r\"\\?+\", \"?\", comment)\n",
        "        return comment\n",
        "\n",
        "    def segmenter(self, input_str):\n",
        "        \"\"\"Performs segmentation using the ekphasis library, it splits 2-3 joined words into proper words\"\"\"\n",
        "        segmented_string = \"\"\n",
        "        for raw_token in input_str.split():\n",
        "           segmented_string += \" \"  +  seg.segment(raw_token)\n",
        "        return segmented_string\n",
        "\n",
        "    def replace_nan(self, input_str):\n",
        "        \"\"\"Replacing nan strings with empty strings - required for textrank\"\"\"        \n",
        "        input_str = re.sub('nan' , '' , str(input_str))\n",
        "        return input_str\n",
        "    \n",
        "    def demojify(self, input_str):\n",
        "        \"\"\"Convert emojis to text - and remove the colon(:)\"\"\"\n",
        "        input_str = emoji.demojize(input_str)\n",
        "        input_str = re.sub(':|_' , ' ' , str(input_str))\n",
        "        # input_str = re.sub('_' , ' ' , str(input_str))\n",
        "        return input_str\n",
        "\n",
        "    def transliterate(self, input_str):\n",
        "        \"\"\"Transliterate back to Devanagari script\"\"\"\n",
        "        doc = nlp(input_str) \n",
        "        detect_language = doc._.language\n",
        "        if detect_language['language'] == 'en' and detect_language['score'] > 0.8:\n",
        "            return input_str\n",
        "        raw_tokens = input_str.split()\n",
        "        transliterated_str = ''\n",
        "        for raw_token in raw_tokens:\n",
        "            try:\n",
        "                transliterated_str += transliterate_dict[raw_token] + ' '\n",
        "            except:\n",
        "                transliterated_str += raw_token + ' '\n",
        "        return transliterated_str\n",
        "    \n",
        "    def replace_hindi_brands(self, input_str):\n",
        "        \"\"\"Replace Hindi mobile brand names by their English counterparts\"\"\"\n",
        "        replaced_str = ''\n",
        "        raw_tokens = input_str.split()\n",
        "        for raw_token in raw_tokens:\n",
        "            try:\n",
        "                replaced_str += devanagari_brands[raw_token] + ' '\n",
        "            except:\n",
        "                replaced_str += raw_token + ' '\n",
        "        return replaced_str\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Reading english - 1grams ...\n",
            "Reading english - 2grams ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAYrTF61cu9p"
      },
      "source": [
        "#### Preprocessing article data and generating the preprocessed csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKuubQApckfH"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "preprocess = False\n",
        "test = True\n",
        "\n",
        "train_datapath_articles = \"/content/Development_Data/Articles/Raw/articles_train_raw.xlsx\"\n",
        "val_datapath_articles = \"/content/Development_Data/Articles/Raw/articles_test_raw.xlsx\"\n",
        "\n",
        "if not test:\n",
        "    if preprocess:\n",
        "        preprocessor = InterIITArticlePreprocessing()      ############################## MODIFY THIS PATH IF NEEDED\n",
        "\n",
        "        train_data_articles = pd.read_excel(train_datapath_articles, engine='openpyxl', usecols=[\"Text\",\"Mobile_Tech_Flag\"])\n",
        "        train_data_articles.columns = [\"text\",\"labels\"]\n",
        "        train_data_articles.drop_duplicates(subset = [\"text\"], inplace = True, ignore_index = True)\n",
        "        train_data_articles = drop_redundant(train_data_articles)\n",
        "        train_data_articles[\"text\"] = train_data_articles['text'].apply(lambda x : preprocessor.run(x))\n",
        "        train_data_articles = POSextraction(train_data_articles)\n",
        "        print_df(train_data_articles)\n",
        "        train_path_articles = '/content/Development_Data/Articles/Transliterated_Nouns/articles_train_transliterated_nouns.xlsx'\n",
        "\n",
        "        val_data_articles = pd.read_excel(val_datapath_articles, engine='openpyxl', usecols=[\"Text\",\"Mobile_Tech_Flag\"])\n",
        "        val_data_articles.columns = [\"text\",\"labels\"]\n",
        "        val_data_articles.drop_duplicates(subset = [\"text\"], inplace = True, ignore_index = True)\n",
        "        val_data_articles = drop_redundant(val_data_articles)       \n",
        "        val_data_articles[\"text\"] = val_data_articles['text'].apply(lambda x : preprocessor.run(x))\n",
        "        val_data_articles = POSextraction(val_data_articles)\n",
        "        print_df(val_data_articles)\n",
        "        val_path_articles = '/content/Development_Data/Articles/Transliterated_Nouns/articles_test_transliterated_nouns.xlsx'\n",
        "\n",
        "        train_data_articles.to_excel(train_path_articles,index=False)\n",
        "        val_data_articles.to_excel(val_path_articles,index=False)\n",
        "\n",
        "    else :\n",
        "        train_path_articles = '/content/Development_Data/Articles/Transliterated_Nouns/articles_train_transliterated_nouns.xlsx'\n",
        "        val_path_articles = '/content/Development_Data/Articles/Transliterated_Nouns/articles_test_transliterated_nouns.xlsx'\n",
        "        train_data_articles = pd.read_excel(train_path_articles, usecols=[\"text\",\"labels\",\"nouns\"])\n",
        "        val_data_articles = pd.read_excel(val_path_articles, usecols=[\"text\",\"labels\",\"nouns\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnI5icWzURZ7"
      },
      "source": [
        "####Preprocessing the test data and generating corresponding tweets and article csvs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPWRUM4nURsc"
      },
      "source": [
        "def get_articles_tweets_separate(df, preprocess):\n",
        "    '''\n",
        "    Separates the articles and tweets from the dataframe and returns two dataframes corresponding to each\n",
        "    Input: dataframe\n",
        "    Output: articles and tweets dataframes\n",
        "    '''\n",
        "    articles_text = []\n",
        "    tweets_text = []\n",
        "    articles_id = []\n",
        "    tweets_id = []\n",
        "    if preprocess == False:\n",
        "        articles_nouns = []\n",
        "        tweets_nouns = []\n",
        "    for i in range(len(df)):\n",
        "        if (df.at[i, 'Text_ID']).startswith('article'):\n",
        "            articles_text.append(df.at[i, 'Text'])\n",
        "            articles_id.append(df.at[i, 'Text_ID'])\n",
        "            if preprocess == False:\n",
        "                articles_nouns.append(df.at[i, 'nouns'])\n",
        "        else:\n",
        "            tweets_text.append(df.at[i, 'Text'])\n",
        "            tweets_id.append(df.at[i, 'Text_ID'])\n",
        "            if preprocess == False:\n",
        "                tweets_nouns.append(df.at[i, 'nouns'])\n",
        "    tweets_df_dict = {'Text_ID': tweets_id, 'Text': tweets_text} if preprocess == True else {'Text_ID': tweets_id, 'Text': tweets_text, 'nouns': tweets_nouns}\n",
        "    articles_df_dict = {'Text_ID': articles_id, 'Text': articles_text} if preprocess == True else {'Text_ID': articles_id, 'Text': articles_text, 'nouns': articles_nouns}\n",
        "    tweets_df = pd.DataFrame(tweets_df_dict)\n",
        "    articles_df = pd.DataFrame(articles_df_dict)\n",
        "    return articles_df, tweets_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntfafBBhUfan",
        "outputId": "ad593e37-5275-4027-896a-fba2a49b6c96"
      },
      "source": [
        "preprocess = True\n",
        "test = True\n",
        "\n",
        "## Preprocessing tweet article data and generating the preprocessed xlsx files\n",
        "if preprocess:\n",
        "    test_path = '/content/Eval_Data/evaluation_raw.xlsx'\n",
        "    preprocessortweet = InterIITTweetPreprocessing()\n",
        "    preprocessorarticle = InterIITArticlePreprocessing()  \n",
        "    df_test = pd.read_excel(test_path, engine = 'openpyxl')\n",
        "    df_test_articles, df_test_tweets = get_articles_tweets_separate(df_test, preprocess = True)\n",
        "    df_test_articles.columns = ['text_id', 'text']\n",
        "    df_test_tweets.columns = ['text_id', 'text']\n",
        "    df_test_tweets[\"text\"] = df_test_tweets['text'].apply(lambda x : preprocessortweet.run(x))\n",
        "    df_test_articles[\"text\"] = df_test_articles['text'].apply(lambda x : preprocessorarticle.run(x))\n",
        "    df_test_tweets = POSextraction(df_test_tweets)\n",
        "    df_test_articles = POSextraction(df_test_articles)\n",
        "    \n",
        "    print_df(df_test_tweets)\n",
        "    print_df(df_test_articles)\n",
        "    \n",
        "    articles_path = '/content/Eval_Data/articles_eval_processed.xlsx'\n",
        "    tweets_path = '/content/Eval_Data/tweets_eval_processed.xlsx'\n",
        "\n",
        "    df_test_articles.to_excel(articles_path, index=False, encoding='UTF-8')\n",
        "    df_test_tweets.to_excel(tweets_path, index=False, encoding='UTF-8')\n",
        "\n",
        "else:\n",
        "    test_path = '/content/Eval_Data/evaluation_processed_nouns.xlsx'\n",
        "    df_test = pd.read_excel(test_path, engine = 'openpyxl')  \n",
        "    df_test_articles, df_test_tweets = get_articles_tweets_separate(df_test, preprocess = False)\n",
        "    print_df(df_test)\n",
        "    articles_path = '/content/Eval_Data/articles_eval_processed.xlsx'\n",
        "    tweets_path = '/content/Eval_Data/tweets_eval_processed.xlsx'\n",
        "    df_test_articles.to_excel(articles_path, index=False, encoding='UTF-8')\n",
        "    df_test_tweets.to_excel(tweets_path, index=False, encoding='UTF-8')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of dataset : (210, 3)\n",
            "Class Distribution : \n",
            "\n",
            "\n",
            "        text_id  ...                                              nouns\n",
            "30   tweet_4031  ...  west brom agree deal ainsley maitland niles lo...\n",
            "172  tweet_4173  ...  xiaomi smartphone display snapdragon g mp rear...\n",
            "84   tweet_4085  ...  kaisi nigra रख रह hai किस app वाले abhi bhi ka...\n",
            "199  tweet_4200  ...  g smartphone जब में नहीं है g नेटवर्क तो क्यों...\n",
            "60   tweet_4061  ...  gt loan repayment plan debt ed month reason re...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "Shape of dataset : (222, 3)\n",
            "Class Distribution : \n",
            "\n",
            "\n",
            "          text_id  ...                                              nouns\n",
            "169  article_4172  ...  © bgr.in poco m 3 india today launch event lau...\n",
            "78   article_4080  ...  मुंबई दिसंबर भारतीय रिजर्व बैंक आरबीआई के पूर्...\n",
            "113  article_4115  ...  loan home loan वालें ध्‍यान दें केस में सुप्री...\n",
            "61   article_4063  ...  l r amad diallo morgan sanson transfers europe...\n",
            "40   article_4041  ...  notificiation holdings globe newswire form not...\n",
            "\n",
            "[5 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5m7__FNeQxo"
      },
      "source": [
        "#### Preprocessing the scraped data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwoXb8gzeRDa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "69501717-f85c-4753-8891-ecbf8faefd98"
      },
      "source": [
        "### Read the file\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/ScrapedData/scraped_articles_preprocessed.csv')\n",
        "\n",
        "### Write the preprocessed data in text file\n",
        "writer = open('/content/ScrapedData/train_data.txt', 'w')\n",
        "for idx in range(int(len(df))):\n",
        "    data = df['Text'][idx]\n",
        "    writer.write(data+'\\n')\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bacd301c3098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Read the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/ScrapedData/scraped_articles_preprocessed.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m### Write the preprocessed data in text file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/ScrapedData/scraped_articles_preprocessed.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nRvm5FDhis3"
      },
      "source": [
        "## Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwX1sZQMd3AJ"
      },
      "source": [
        "### Download Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUM3-tUEdvYB",
        "outputId": "88101241-cdc0-4555-b27b-5355f572ffd1"
      },
      "source": [
        "### Install dependencies\n",
        "%cd \"/content/\"\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "\n",
        "%cd \"/content/transformers/\"\n",
        "!pip install .\n",
        "!pip install -r ./examples/language-modeling/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 67550 (delta 12), reused 19 (delta 12), pack-reused 67529\u001b[K\n",
            "Receiving objects: 100% (67550/67550), 50.19 MiB | 21.48 MiB/s, done.\n",
            "Resolving deltas: 100% (48020/48020), done.\n",
            "/content/transformers\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (3.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 16.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (1.24.3)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.5.0.dev0-cp37-none-any.whl size=1973230 sha256=2775478307288d1b847452ebb9a02267b11aefd586239acf66efdaa1c1736a41\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-31rm6x_9/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=f8f8357a906ab481c10e3eb1612958904ffa0095cbabb3bd00d30c64751877dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.5.0.dev0\n",
            "Collecting datasets>=1.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/43b396481a8298c6010afb93b3c1e71d4ba6f8c10797a7da8eb005e45081/datasets-1.5.0-py3-none-any.whl (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 10.0MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 21.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r ./examples/language-modeling/requirements.txt (line 3)) (3.12.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (0.3.3)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/0d/a6bfee0ddf47b254286b9bd574e6f50978c69897647ae15b14230711806e/fsspec-0.8.7-py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (3.7.2)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (1.1.5)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/af/07/bf95f398e6598202d878332280f36e589512174882536eb20d792532a57d/huggingface_hub-0.0.7-py3-none-any.whl\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/27/1c0b37c53a7852f1c190ba5039404d27b3ae96a55f48203a74259f8213c9/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (0.70.11.1)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r ./examples/language-modeling/requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->-r ./examples/language-modeling/requirements.txt (line 3)) (54.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r ./examples/language-modeling/requirements.txt (line 1)) (3.0.12)\n",
            "Installing collected packages: fsspec, huggingface-hub, xxhash, datasets, sentencepiece\n",
            "Successfully installed datasets-1.5.0 fsspec-0.8.7 huggingface-hub-0.0.7 sentencepiece-0.1.95 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui2rqtPHe6Oj"
      },
      "source": [
        "### Perform Masked Language modeling on preprocessed scraped data and generate the checkpoint  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxsEG_V8fbsv",
        "outputId": "0b60e998-b4a3-4b0a-e5c8-85ced05e7a36"
      },
      "source": [
        "TRAIN_LM_MODEL = True\n",
        "LM_MODEL_PATH = None\n",
        "\n",
        "\n",
        "if TRAIN_LM_MODEL: ### Run the MLM script from transformers (Takes 2 hours - only run if TRAIN_LM_MODEL is True)\n",
        "    !python ./transformers/examples/language-modeling/run_mlm.py \\\n",
        "        --output_dir=hinglish_synthetic1 \\\n",
        "        --overwrite_output_dir \\\n",
        "        --model_type=roberta \\\n",
        "        --model_name_or_path=xlm-roberta-base \\\n",
        "        --per_device_train_batch_size=2 \\\n",
        "        --per_device_eval_batch_size=8 \\\n",
        "        --do_train \\\n",
        "        --save_total_limit 2 \\\n",
        "        --train_file='/content/ScrapedData/train_data.txt' \\\n",
        "        --save_steps 4500\n",
        "else: # Downloading the already Pretrained MLM Model:\n",
        "    ! wget -q \"https://www.dropbox.com/s/aczakpcbvu4gtg7/pytorch_model.bin?dl=1\" -O \"/content/pytorch_mlm_model.bin\"\n",
        "%cd \"/content/\"\n",
        "\n",
        "if TRAIN_LM_MODEL:\n",
        "    LM_MODEL_PATH = '/transformers/hinglish_synthetic/pytorch_model.bin'\n",
        "else:\n",
        "    LM_MODEL_PATH = '/content/pytorch_mlm_model.bin'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-23 14:42:25.241297: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "03/23/2021 14:42:26 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "03/23/2021 14:42:26 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=hinglish_synthetic1, overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Mar23_14-42-26_dc0b80ac8709, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=4500, save_total_limit=2, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=hinglish_synthetic1, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
            "03/23/2021 14:42:27 - WARNING - datasets.builder -   Using custom data configuration default-6e65076e6c34a4d8\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-6e65076e6c34a4d8/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-6e65076e6c34a4d8/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1392] 2021-03-23 14:42:27,785 >> https://huggingface.co/xlm-roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpasgjt6gm\n",
            "Downloading: 100% 512/512 [00:00<00:00, 365kB/s]\n",
            "[INFO|file_utils.py:1396] 2021-03-23 14:42:28,061 >> storing https://huggingface.co/xlm-roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
            "[INFO|file_utils.py:1399] 2021-03-23 14:42:28,061 >> creating metadata file for /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
            "[INFO|configuration_utils.py:472] 2021-03-23 14:42:28,062 >> loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
            "[INFO|configuration_utils.py:508] 2021-03-23 14:42:28,062 >> Model config XLMRobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:472] 2021-03-23 14:42:28,331 >> loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
            "[INFO|configuration_utils.py:508] 2021-03-23 14:42:28,332 >> Model config XLMRobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1392] 2021-03-23 14:42:28,662 >> https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7b4jbppe\n",
            "Downloading: 100% 5.07M/5.07M [00:00<00:00, 7.34MB/s]\n",
            "[INFO|file_utils.py:1396] 2021-03-23 14:42:29,662 >> storing https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model in cache at /root/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
            "[INFO|file_utils.py:1399] 2021-03-23 14:42:29,662 >> creating metadata file for /root/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
            "[INFO|file_utils.py:1392] 2021-03-23 14:42:30,009 >> https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_slczvte\n",
            "Downloading: 100% 9.10M/9.10M [00:00<00:00, 10.2MB/s]\n",
            "[INFO|file_utils.py:1396] 2021-03-23 14:42:31,250 >> storing https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
            "[INFO|file_utils.py:1399] 2021-03-23 14:42:31,250 >> creating metadata file for /root/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-03-23 14:42:32,047 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-03-23 14:42:32,047 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-03-23 14:42:32,047 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-03-23 14:42:32,047 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-03-23 14:42:32,048 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|file_utils.py:1392] 2021-03-23 14:42:32,912 >> https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpf4x90xtq\n",
            "Downloading: 100% 1.12G/1.12G [00:26<00:00, 42.1MB/s]\n",
            "[INFO|file_utils.py:1396] 2021-03-23 14:42:59,501 >> storing https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
            "[INFO|file_utils.py:1399] 2021-03-23 14:42:59,501 >> creating metadata file for /root/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
            "[INFO|modeling_utils.py:1051] 2021-03-23 14:42:59,502 >> loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
            "[INFO|modeling_utils.py:1167] 2021-03-23 14:43:14,545 >> All model checkpoint weights were used when initializing XLMRobertaForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:1176] 2021-03-23 14:43:14,546 >> All the weights of XLMRobertaForMaskedLM were initialized from the model checkpoint at xlm-roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForMaskedLM for predictions without further training.\n",
            "[WARNING|tokenization_utils_base.py:3134] 2021-03-23 14:43:14,556 >> Token indices sequence length is longer than the specified maximum sequence length for this model (708 > 512). Running this sequence through the model will result in indexing errors\n",
            "100% 15/15 [00:12<00:00,  1.24ba/s]\n",
            "100% 15/15 [00:39<00:00,  2.62s/ba]\n",
            "[INFO|trainer.py:485] 2021-03-23 14:44:15,081 >> The following columns in the training set  don't have a corresponding argument in `XLMRobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:979] 2021-03-23 14:44:15,303 >> ***** Running training *****\n",
            "[INFO|trainer.py:980] 2021-03-23 14:44:15,303 >>   Num examples = 6250\n",
            "[INFO|trainer.py:981] 2021-03-23 14:44:15,303 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:982] 2021-03-23 14:44:15,303 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:983] 2021-03-23 14:44:15,303 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:984] 2021-03-23 14:44:15,304 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:985] 2021-03-23 14:44:15,304 >>   Total optimization steps = 9375\n",
            "  1% 57/9375 [01:20<3:40:45,  1.42s/it]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5h_khLAEWGI"
      },
      "source": [
        "## Mobile Tech Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPYY93GbjL7w",
        "outputId": "735316ee-89b4-41a5-8ae7-fb0b2a6c0ed6"
      },
      "source": [
        "!pip install pytorch_lightning\n",
        "!pip install transformers\n",
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.7/dist-packages (1.2.4)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.8.0+cu101)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.4.1)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (5.3.1)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.8.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch_lightning) (3.7.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (54.1.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.36.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.27.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (3.7.2)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (3.7.4.post0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.4.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (20.3.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.6.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (5.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAnBC_g2EazY"
      },
      "source": [
        "### Vanilla Classification model based on huggingface and lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixvo83YkGh0Q"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "############################ Dataset for Train,Test ############################\n",
        "\n",
        "class Dataset():\n",
        "    def __init__(self,text,labels,maxlen,tokenizer):\n",
        "        self.text =text\n",
        "        self.target = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.maxlen = maxlen\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "    \n",
        "    def __getitem__(self,index):        \n",
        "        input_bert = self.tokenizer.encode_plus(self.text[index],None,add_special_tokens=True,max_length=self.maxlen,padding=\"max_length\",truncation=True)\n",
        "        ids = input_bert[\"input_ids\"]\n",
        "        attn_mask = input_bert[\"attention_mask\"]\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"attn_mask\": torch.tensor(attn_mask, dtype=torch.float),\n",
        "            \"target\": torch.tensor(self.target[index], dtype=torch.long)\n",
        "        }\n",
        "    \n",
        "\n",
        "class TestDataset():\n",
        "    def __init__(self,text,maxlen,tokenizer):\n",
        "        self.text =text\n",
        "        self.tokenizer = tokenizer\n",
        "        self.maxlen = maxlen\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "    \n",
        "    def __getitem__(self,index):        \n",
        "        input_bert = self.tokenizer.encode_plus(self.text[index],None,add_special_tokens=True,max_length=self.maxlen,padding=\"max_length\",truncation=True)\n",
        "        ids = input_bert[\"input_ids\"]\n",
        "        attn_mask = input_bert[\"attention_mask\"]\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"attn_mask\": torch.tensor(attn_mask, dtype=torch.float),\n",
        "            \"text_idx\":index\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj2a3KmsEoJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79088a4-73cb-45ff-be81-3003178327e7"
      },
      "source": [
        "!pip install pytorch_lightning\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.7/dist-packages (1.2.4)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.8.0+cu101)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.8.7)\n",
            "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (5.3.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.36.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (54.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch_lightning) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (3.7.2)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (3.7.4.post0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.4.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (20.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (5.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.6.3)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oHABTZxjrXU"
      },
      "source": [
        "df_test_articles = pd.read_excel(\"/content/Eval_Data/Articles/Transliterated_Nouns/articles_eval_processed.xlsx\")\n",
        "df_test_tweets = pd.read_excel(\"/content/Eval_Data/Tweets/Transliterated_Nouns/tweets_eval_processed.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNOXG3GGZbq2"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM, get_linear_schedule_with_warmup, AdamW\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "########################### Model Class #################################\n",
        "\n",
        "class Classification(pl.LightningModule):\n",
        "    def __init__(self,config,tokenizer):\n",
        "        super(Classification, self).__init__()\n",
        "        self.maxlen = config[\"maxlen\"]\n",
        "        self.hidden = config[\"hidden_size\"]\n",
        "        self.train_batch_size = config[\"train_batch_size\"]\n",
        "        self.val_batch_size = config[\"val_batch_size\"]\n",
        "        self.epochs = config[\"epochs\"]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.embeddings = AutoModelForMaskedLM.from_pretrained(config[\"BERT_PATH\"])\n",
        "        self.embeddings.load_state_dict(torch.load(config[\"model_path\"], device))\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear = nn.Linear(config[\"hidden_size\"]*2, 2)\n",
        "        self.lr=config[\"lr\"]\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None,return_dict=False):\n",
        "        out, hn = self.embeddings.roberta(input_ids, attention_mask=attention_mask,return_dict=return_dict)\n",
        "        mean_pooling = torch.mean(out, 1)\n",
        "        max_pooling, _ = torch.max(out, 1)\n",
        "        embed = torch.cat((mean_pooling, max_pooling), 1)\n",
        "        y_pred = self.linear(self.dropout(embed))\n",
        "        return y_pred\n",
        "\n",
        "    def setup(self,stage):\n",
        "        self.df_train = pd.read_excel(config[\"train_path\"],usecols=[\"text\",\"labels\"])\n",
        "        self.df_valid = pd.read_excel(config[\"val_path\"],usecols=[\"text\",\"labels\"])\n",
        "        self.num_training_steps = int(len(self.df_train)/self.train_batch_size * self.epochs)\n",
        "        self.val_preds=[]\n",
        "        self.val_targets=[]\n",
        "        self.test_preds=[]\n",
        "        self.test_idxs=[]\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        train_dataset = Dataset(self.df_train.text.tolist(),self.df_train.labels.values,self.maxlen,self.tokenizer)\n",
        "        train_data_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=self.train_batch_size,num_workers=4)\n",
        "        return train_data_loader        \n",
        "\n",
        "    def val_dataloader(self):\n",
        "        valid_dataset = Dataset(self.df_valid.text.tolist(),self.df_valid.labels.values,self.maxlen,self.tokenizer)\n",
        "        valid_data_loader = torch.utils.data.DataLoader(valid_dataset,shuffle=False, batch_size=self.val_batch_size,num_workers=4)\n",
        "        return valid_data_loader\n",
        "    \n",
        "    def test_step(self,batch,batch_idx):\n",
        "        id,attn_mask,index = batch[\"ids\"] , batch[\"attn_mask\"], batch[\"text_idx\"]\n",
        "        y_pred = self(id,attn_mask)\n",
        "        preds = torch.argmax(y_pred,dim=1)\n",
        "        self.test_preds.extend(preds.cpu().detach().numpy().tolist())\n",
        "        self.test_idxs.extend(index)\n",
        "\n",
        "    def training_step(self,batch,batch_idx):\n",
        "        id,attn_mask,target = batch[\"ids\"] , batch[\"attn_mask\"], batch[\"target\"]\n",
        "        y_pred = self(id,attn_mask)\n",
        "        loss = self.criterion(y_pred,target)\n",
        "        self.log(\"train loss step\",loss,prog_bar=True,logger=False)\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        id,attn_mask,target = batch[\"ids\"] , batch[\"attn_mask\"], batch[\"target\"]\n",
        "        y_pred = self(id,attn_mask)\n",
        "        loss = self.criterion(y_pred,target)\n",
        "        preds = torch.argmax(y_pred,dim=1)\n",
        "        self.val_preds.extend(preds.cpu().detach().numpy().tolist())\n",
        "        self.val_targets.extend(target.cpu().numpy().tolist())\n",
        "        return {\"val_loss\": loss}\n",
        "    \n",
        "    def validation_epoch_end(self, val_losses):\n",
        "        avg_val_loss = torch.stack([x[\"val_loss\"] for x in val_losses]).mean()\n",
        "        accuracy = metrics.accuracy_score(self.val_targets, self.val_preds) \n",
        "        F1_score = metrics.f1_score(self.val_targets, self.val_preds)\n",
        "        print(\"---------------------End of Epoch----------------\")\n",
        "        print(\"Validation loss : {}\".format(avg_val_loss))\n",
        "        print(\"Validation Accuracy : {}\".format(accuracy))\n",
        "        print(\"Validation F1 score : {}\".format(F1_score))\n",
        "        print(\"-----------------------------------------------\")\n",
        "        self.val_preds=[]\n",
        "        self.val_targets=[]\n",
        "        self.log(\"avg val loss\",avg_val_loss,prog_bar=True,logger=True)\n",
        "        \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(), lr=self.lr)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=2, num_training_steps=self.num_training_steps) \n",
        "        return [optimizer] , [scheduler]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pk49IiZkGFv"
      },
      "source": [
        "import torch\n",
        "########################### Config for Model ####################################\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Important parameters for Training\n",
        "#Used only for instantiating model class during test\n",
        "test_path=\"\"\n",
        "train_path=\"\"\n",
        "config={\n",
        "\"maxlen\" : 512 ,\n",
        "\"train_batch_size\" : 8,\n",
        "\"val_batch_size\" : 8,\n",
        "\"epochs\" : 3,\n",
        "\"BERT_PATH\" : 'xlm-roberta-base',\n",
        "\"model_path\" : '/content/pytorch_mlm_model.bin',\n",
        "\"lr\" : 2e-5,\n",
        "\"hidden_size\":768,\n",
        "\"device\": device,\n",
        "\"train_path\":train_path ,\n",
        "\"val_path\" : test_path\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNlKXcCiTGLt"
      },
      "source": [
        "########################### Training ###################################\n",
        "test=True\n",
        "if not test:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config[\"BERT_PATH\"])\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath='Models/',\n",
        "        verbose=True,\n",
        "        save_last=True\n",
        "    )\n",
        "    torch.cuda.empty_cache()\n",
        "    model = Classification(config,tokenizer)\n",
        "    trainer = Trainer(max_epochs=config[\"epochs\"],gpus=1,fast_dev_run=False,checkpoint_callback=checkpoint_callback)\n",
        "    trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "sh97GUWjkcvQ",
        "outputId": "296aaa75-6189-439e-a8b5-a0fa5f6378c4"
      },
      "source": [
        "#################################### Downloading Checkpoints for Classification - Articles and Tweets ##############33\n",
        "import gdown\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?id=1VBNRX_uLzx8zSa_Jzsz6InvR9WW9321f', \"Tweets.ckpt\", quiet=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VBNRX_uLzx8zSa_Jzsz6InvR9WW9321f\n",
            "To: /content/Tweets.ckpt\n",
            "3.33GB [00:47, 70.2MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tweets.ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "-fOVyMywke-G",
        "outputId": "437a9367-bce1-4e6d-a4bc-dec3ca5b201f"
      },
      "source": [
        "\n",
        "gdown.download('https://drive.google.com/uc?id=1X4zVhiVsk-I8edxLDwElDr1w8P3do_wT', \"Articles.ckpt\", quiet=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1X4zVhiVsk-I8edxLDwElDr1w8P3do_wT\n",
            "To: /content/Articles.ckpt\n",
            "3.33GB [01:17, 42.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Articles.ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604,
          "referenced_widgets": [
            "21ce3e7658b54e39ac3fd5d36d4988ea",
            "25e6572dac304d25904e1cdd9f1c24ab",
            "cc86f275dff54b9ca4a990f3f7cf6aab",
            "45b279f00c4a41d3bd4211298a04880d",
            "29ee818a97d241d1bf4506561fbb7677",
            "e1e8373d5c3e4bc0bb5ae7c9ee8942c4",
            "aa16ef3750a3494282bd357f1f630975",
            "8aa2e2c70bca44e1b132e7452e9174a6",
            "51a013cac54e44c4b464cf030feec4fe",
            "fd9132ec9b924bbd82946cb70b721f4d",
            "c62b4f15067045e880509e24571d43b2",
            "8410f01fa86240caba0980743e2b8e05",
            "26cd7b1e3e04477982f7fc0657a62ab9",
            "40233e3a22eb48d98f48bbd2118a0dc6",
            "00b567cd8b94449eb5629f9dfcd94197",
            "aec63575b01e4a2c91ca7e3a8b51a144",
            "43c1751201dd46efbf5ebf7d912d3617",
            "630e43e743334a00881def84e4f39f3e",
            "d69bac052879463dbb8e200e5c2cdbcb",
            "ae21ea59760240618a715a50a48bf43f",
            "b9aaf5913e0d4e37b22166fc15feb6bc",
            "112e667d0805470e878c912abab7f972",
            "8fe8aced2d174df8ae938959320eb6e1",
            "0105488cfb44408aa66396e2baea574d",
            "1a87d9626bfc451395181ff4be231622",
            "26a2d72dc3d4440aa9d05f9358249158",
            "4d4e6e916f7b4d7fbcd64d1a772ea08a",
            "9cb52343a24749a1977774f46806fae2",
            "f65d4b1cac98448292172943b1effeeb",
            "f788116ceb2a47468f6cb49ad92e6b8f",
            "56857d0d383b4fa9862f5b1ce3d17b28",
            "f5acc16fe8ee46d690a604fd2013c2a4"
          ]
        },
        "id": "xBzTRea5ai5O",
        "outputId": "1bfb4243-42d3-4a2c-fb0c-6456ed35ad66"
      },
      "source": [
        "######################### Testing #####################################\n",
        "tokenizer = AutoTokenizer.from_pretrained(config[\"BERT_PATH\"])\n",
        "\n",
        "def test_model(ckpt_path,tokenizer,df_test,maxlen,type,test_batch_size):\n",
        "    print(\"Getting results for  {} test set\".format(type))\n",
        "    test_dataset = TestDataset(df_test.Text.tolist(),maxlen,tokenizer)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_dataset,shuffle=False, batch_size=test_batch_size)\n",
        "    trainer = Trainer(gpus=1)\n",
        "    model_test = Classification.load_from_checkpoint(ckpt_path,config=config,tokenizer=tokenizer) \n",
        "    trainer.test(model=model_test, test_dataloaders=test_dataloader)\n",
        "    df_test[\"labels\"] = model_test.test_preds\n",
        "    return df_test\n",
        "\n",
        "\n",
        "######################## Articles and Tweets Generating Predictions##################################\n",
        "checkpoint_path_article=\"Articles.ckpt\"\n",
        "checkpoint_path_tweets=\"Tweets.ckpt\"\n",
        "df_test_tweets = test_model(checkpoint_path_tweets,tokenizer,df_test_tweets,128,\"Tweets\",8)\n",
        "df_test_articles = test_model(checkpoint_path_article,tokenizer,df_test_articles,512,\"Articles\",8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21ce3e7658b54e39ac3fd5d36d4988ea",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=512.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51a013cac54e44c4b464cf030feec4fe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43c1751201dd46efbf5ebf7d912d3617",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9096718.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting results for  Tweets test set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a87d9626bfc451395181ff4be231622",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1115590446.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-8dbebd4dd8de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcheckpoint_path_article\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Articles.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mcheckpoint_path_tweets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Tweets.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf_test_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path_tweets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_test_tweets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Tweets\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdf_test_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path_article\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_test_articles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Articles\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-8dbebd4dd8de>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(ckpt_path, tokenizer, df_test, maxlen, type, test_batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHECKPOINT_HYPER_PARAMS_KEY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36m_load_model_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0m_cls_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_cls_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls_init_args_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_cls_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# give model a chance to load something\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-39705e30dcfe>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, tokenizer)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"BERT_PATH\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hidden_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/pytorch_mlm_model.bin'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrRhyjcTkucM"
      },
      "source": [
        "!rm -rf Articles.ckpt\n",
        "!rm -rf Tweets.ckpt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8POeuCoyKO19"
      },
      "source": [
        "## Brand Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MptgG9LLLmop",
        "outputId": "6615d3df-a51f-43fd-91ee-2385db9d11a8"
      },
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "import re\n",
        "import nltk\n",
        "import time\n",
        "import emoji\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import concurrent.futures\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "import unicodedata\n",
        "import json\n",
        "\n",
        "eng_brand_df = pd.read_csv(\"/content/eng_brands.csv\")                             # add the wget link for this\n",
        "eng_brand_df[\"brands\"] = eng_brand_df[\"brands\"].apply(lambda x: x.lower())\n",
        "brands_list = list(set(eng_brand_df[\"brands\"].tolist()))\n",
        "\n",
        "brands = []\n",
        "with open(\"/content/eng.json\") as f:\n",
        "\tbrands = json.load(f)\n",
        "\n",
        "brands = brands + brands_list\n",
        "brands = list(set(brands))\n",
        "brands=[x.lower() for x in brands]\n",
        "\n",
        "df_hindi_brands = pd.read_excel(\"/content/Hindi-brands.xlsx\")\n",
        "\n",
        "hindi_brands = {}\n",
        "with open(\"/content/hbrands2.json\") as f:\n",
        "    hindi_brands=json.load(f)\n",
        "\n",
        "df_tweet = pd.read_excel(\"/content/Eval_Data/tweets_eval_processed.xlsx\")\n",
        "df_articles = pd.read_excel(\"/content/Eval_Data/articles_eval_processed.xlsx\")\n",
        "\n",
        "def fuzzy_match(string):\n",
        "\textracted_ratio = []\n",
        "\twords = string.split(\" \")\n",
        "\tfor i, word in enumerate(words):\n",
        "\t\tif word in hindi_brands.keys():\n",
        "\t\t\textracted_ratio.append((word, hindi_brands[word]))\n",
        "\t\t\twords[i] = \" \"\n",
        "\tthresh = 95\n",
        "\tfor word in words:\n",
        "\t\tif len(word) < 3:\n",
        "\t\t\tcontinue\n",
        "\t\tmaxval = 0\n",
        "\t\tmaxword = \"\"\n",
        "\t\tfor brand in brands:\n",
        "\n",
        "\t\t\tval = fuzz.ratio(brand.lower(), word.lower())\n",
        "\t\t\tif val > thresh:\n",
        "\t\t\t\tmaxword = brand\n",
        "\t\t\t\tmaxval = val\n",
        "\t\tif maxval > thresh:\n",
        "\t\t\textracted_ratio.append((word, maxword))\n",
        "\n",
        "\treturn extracted_ratio\n",
        "\n",
        "with concurrent.futures.ProcessPoolExecutor() as pool:\n",
        "\tdf_tweet[f\"abs_95\"] = list(\n",
        "\t\ttqdm.tqdm(\n",
        "\t\t\tpool.map(fuzzy_match, df_test_tweets[\"nouns\"], chunksize=10),\n",
        "\t\t\ttotal=df_tweet.shape[0],\n",
        "\t\t)\n",
        "\t)\n",
        " \n",
        "with concurrent.futures.ProcessPoolExecutor() as pool:\n",
        "\tdf_articles[f\"abs_95\"] = list(\n",
        "\t\ttqdm.tqdm(\n",
        "\t\t\tpool.map(fuzzy_match, df_test_articles[\"nouns\"], chunksize=10),\n",
        "\t\t\ttotal=df_articles.shape[0],\n",
        "\t\t)\n",
        "\t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "145 27\n",
            "210\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 210/210 [00:08<00:00, 25.38it/s]\n",
            " 77%|███████▋  | 171/222 [01:48<00:22,  2.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "this shouldnt happen but it happened\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 222/222 [02:36<00:00,  1.42it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SICeSdl6O3Gw"
      },
      "source": [
        "df_tweet.to_excel(f\"all_evaluation_tweet_data.xlsx\")\n",
        "df_articles.to_excel(f\"all_evaluation_articles_data.xlsx\")\n",
        "\n",
        "df_articles_tweets = pd.concat((df_articles, df_tweet))\n",
        "df_articles_tweets.to_excel(f\"all_evaluation_data.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CphzE8YvIsHx"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dZUZIs49Jehf",
        "outputId": "6e74cd36-35a0-4eef-eaf6-9d17e518db06"
      },
      "source": [
        "# necessary installs\n",
        "!pip install spacy==2.1.0\n",
        "!pip install spacy_langdetect\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install https://github.com/explosion/spacy-models/releases//download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz\n",
        "!pip install neuralcoref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy==2.1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/78/0f/ca790def675011f25bce8775cf9002b5085cd2288f85e891f70b32c18752/spacy-2.1.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.19.5)\n",
            "Collecting srsly<1.1.0,>=0.0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/48/44bd8693a7a705976884cabd52a516fe5cbcecf5f45be732f8f04ad2605b/srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.23.0)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "  Using cached https://files.pythonhosted.org/packages/fa/5f/47b7b29ad202b2210020e2f33bfb06d1db2abe0e709c2a84736e8a9d1bd5/blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Collecting thinc<7.1.0,>=7.0.2\n",
            "  Using cached https://files.pythonhosted.org/packages/36/42/d7ea7539af3852fd8c1f0b3adf4a100fb3d72b40b69cef1a764ff979a743/thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "  Using cached https://files.pythonhosted.org/packages/bc/2b/3ecd5d90d2d6fd39fbc520de7d80db5d74defdc2d7c2e15531d9cc3498c7/preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (0.9.6)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.41.1)\n",
            "\u001b[31mERROR: en-core-web-sm 3.0.0 has requirement spacy<3.1.0,>=3.0.0, but you'll have spacy 2.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: srsly, blis, preshed, thinc, spacy\n",
            "  Found existing installation: srsly 2.4.0\n",
            "    Uninstalling srsly-2.4.0:\n",
            "      Successfully uninstalled srsly-2.4.0\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.5\n",
            "    Uninstalling preshed-3.0.5:\n",
            "      Successfully uninstalled preshed-3.0.5\n",
            "  Found existing installation: thinc 8.0.2\n",
            "    Uninstalling thinc-8.0.2:\n",
            "      Successfully uninstalled thinc-8.0.2\n",
            "  Found existing installation: spacy 3.0.5\n",
            "    Uninstalling spacy-3.0.5:\n",
            "      Successfully uninstalled spacy-3.0.5\n",
            "Successfully installed blis-0.2.4 preshed-2.0.1 spacy-2.1.0 srsly-1.0.5 thinc-7.0.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "blis",
                  "preshed",
                  "spacy",
                  "srsly",
                  "thinc"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy_langdetect in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from spacy_langdetect) (3.6.4)\n",
            "Requirement already satisfied: langdetect==1.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy_langdetect) (1.0.7)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (20.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (1.15.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (8.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (54.1.2)\n",
            "Collecting en_core_web_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 13.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-cp37-none-any.whl size=11074434 sha256=543d7ae02bd1ee646a86d2d1e196352c820a8c98773e138ce2192da8c6887379\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u8q_eeo9/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 3.0.0\n",
            "    Uninstalling en-core-web-sm-3.0.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.0.0\n",
            "Successfully installed en-core-web-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Collecting https://github.com/explosion/spacy-models/releases//download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases//download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
            "\u001b[K     |████████████████████████████████| 826.9MB 22kB/s \n",
            "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): en-core-web-lg==2.1.0 from https://github.com/explosion/spacy-models/releases//download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==2.1.0) (2.1.0)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (0.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (7.0.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (0.8.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (2.0.5)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (1.19.5)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (2.6.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->en-core-web-lg==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->en-core-web-lg==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->en-core-web-lg==2.1.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->en-core-web-lg==2.1.0) (2.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->en-core-web-lg==2.1.0) (4.41.1)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.1.0-cp37-none-any.whl size=828255077 sha256=8c1bd95e673de63f10e406dc97655f70fb3d6f425e514388b1e2e5c2c906eb7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/23/48/c3271dd3a62b4dbe0edc676eca71ca861cf8d985675438d3dc\n",
            "Successfully built en-core-web-lg\n",
            "Collecting neuralcoref\n",
            "  Using cached https://files.pythonhosted.org/packages/06/6d/c90e5bfd1b8ef32f1b231a32f2f625bf33df7525324d2bbcd08992791d64/neuralcoref-4.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (1.19.5)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.1.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (1.17.35)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.8.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.5)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.35 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref) (1.20.35)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref) (0.3.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.35->boto3->neuralcoref) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.35->boto3->neuralcoref) (1.15.0)\n",
            "Installing collected packages: neuralcoref\n",
            "Successfully installed neuralcoref-4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "neuralcoref"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dyPr9mYJfR7"
      },
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import os\n",
        "import ast\n",
        "import json\n",
        "import copy\n",
        "import spacy\n",
        "import pickle\n",
        "# import langid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import xml.etree.ElementTree\n",
        "from langdetect import detect\n",
        "from textblob import TextBlob \n",
        "# from indictrans import Transliterator\n",
        "spacy.load('en_core_web_sm')\n",
        "import en_core_web_lg\n",
        "import neuralcoref\n",
        "\n",
        "from spacy_langdetect import LanguageDetector\n",
        "nlp = en_core_web_lg.load()\n",
        "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
        "neuralcoref.add_to_pipe(nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aptXToNDJjqm"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from transformers import AutoModel, AutoTokenizer\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import copy\n",
        "# from transformers import RobertaModel, BertTokenizer, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, random_split, DataLoader, IterableDataset, ConcatDataset\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykX2Kk4fJkXn"
      },
      "source": [
        "BASE_DIR = \"\"\n",
        "TEST_DF_FILE_PATH = os.path.join(BASE_DIR, f\"evaluation_with_seperate_brands.csv\") ##\n",
        "BRAND_LABEL_COL = \"abs_95\"\n",
        "#SENTIMENT_LABEL_COL = \"label_abs_95\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76l4DLBHJnlV"
      },
      "source": [
        "# transliterator = Transliterator(source='hin', target='eng', build_lookup=True)\n",
        "sentiment2mams_sentiment={'pos':'positive','neg':'negative','neu':'neutral'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTKCW24-Jp2j"
      },
      "source": [
        "def filter_brands(comment):\n",
        "    \"\"\"\n",
        "    comment contains a list of brands present in a doc as a single string.\n",
        "    this function filters the string, removes punctuations and returns a final\n",
        "    set of distinct brand names present in doc\n",
        "\n",
        "    returns: set(brand names)\n",
        "    \"\"\"\n",
        "    comment = re.sub(\n",
        "    r\"[\\*\\\"“”\\|\\~\\n\\\\\\@\\{\\}\\%\\_\\#\\^\\&\\$\\£\\+\\<\\>\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", # stripping\n",
        "    str(comment))\n",
        "    comment = re.sub(r\"[ ]+\", \"\", comment) # punct filters\n",
        "    comment = re.sub(r\"\\n\" \"+\", \"\", comment)\n",
        "    comment = re.sub(r\"\\!+\", \"!\", comment)\n",
        "    comment = re.sub(r\"\\,+\", \",\", comment)\n",
        "    comment = re.sub(r\"\\?+\", \"?\", comment)\n",
        "    new = comment.split(',') # splitting the brands separated by comma\n",
        "    New = [word.strip('\\'') for word in new]\n",
        "    ss = set(New) # storing them in set to remove duplicates\n",
        "    return ss\n",
        "\n",
        "def get_branded_sentences(ss, resolved_doc, merge_sent =True ):\n",
        "\n",
        "    \"\"\"\n",
        "    This function performs string matching on the doc using the set(brand names) obtained from \n",
        "    filter_brands()\n",
        "\n",
        "    Args:\n",
        "        params:\n",
        "          ss: set of brands\n",
        "          resolved_docs: documentation after resolving the brand entities usinf coreference\n",
        "        returns:\n",
        "          dicti : a dictionary with keys as the unique brand names present in doc\n",
        "                  and the corresponding values are the sentences with the resepctive brand mentions\n",
        "                  separated by a dot(.)\n",
        "\n",
        "    \"\"\"\n",
        "    dicti = {}\n",
        "    for i in ss:\n",
        "      dicti[i] = None\n",
        "    for brand in ss: # for every unique brand in doc\n",
        "      lis = []\n",
        "      for sentence in resolved_doc.split('.'):  # for each sentence in doc\n",
        "          if brand in sentence: # if sentence has the brand\n",
        "            lis.append(sentence) \n",
        "      dicti[brand] = lis # store all the separated sentences as a key in the brand dict\n",
        "    if merge_sent:\n",
        "        for ent in dicti.keys():\n",
        "          dicti[ent] = ['.'.join(dicti[ent])] # join the sentences while keeping them separated with a \".\"\n",
        "    return dicti\n",
        "\n",
        "def coref(text): # uses spacy neural coref to resolve the given doc\n",
        "    doc = nlp(text)\n",
        "    resolved_doc = doc._.coref_resolved # coreference resolving\n",
        "    return resolved_doc\n",
        "\n",
        "def get_df(text, brands):\n",
        "    \"\"\"\n",
        "    converts a text (doc) and all the fuzzy extracted brand names in the doc to \n",
        "    a pandas dataframe with each row containing sentences pertaining to a single brand\n",
        "    \"\"\"\n",
        "    resolved_doc = coref(text)\n",
        "    ss = filter_brands(brands)  \n",
        "    dicti = get_branded_sentences(ss, resolved_doc ) \n",
        "    df2 = pd.DataFrame.from_dict(dicti, orient='index') # converts the dicti to a df\n",
        "    return df2\n",
        "  \n",
        "def make_extracted_df(parent_df):\n",
        "    \"\"\"\n",
        "    iterates through all the rows in df, performs a language detection\n",
        "    concatenates the dataframe obtained for each row and returns a final dataframe\n",
        "    containing all the samples (i.e, brand -> sentences)\n",
        "    \"\"\"\n",
        "    \n",
        "    PATH = \"all_evaluation_data.xlsx\"\n",
        "    count = 0\n",
        "    fuzzy_df=pd.read_excel(os.path.join(BASE_DIR, PATH))\n",
        "    for i in range(len(fuzzy_df)):\n",
        "        text = fuzzy_df.at[i, 'text']\n",
        "        brands = fuzzy_df.iloc[i, :]['abs_95']\n",
        "        doc = nlp(text) \n",
        "        detect_language = doc._.language # performs language detetction\n",
        "        if detect_language['language'] == 'en' and detect_language['score'] > 0.8:\n",
        "            count += 1\n",
        "            sample_df = get_df(text, brands) \n",
        "            sample_df[\"ori_text\"] = text # save original doc for each sample\n",
        "            parent_df = pd.concat([parent_df, sample_df], axis = 0) # concatenate the df obtained from each sample\n",
        "    return parent_df\n",
        "\n",
        "available = True\n",
        "\n",
        "if not available:\n",
        "    init = pd.DataFrame(['A'])                          # init DataFrame \n",
        "    final_df = make_extracted_df(init)                  # create the final df\n",
        "    final_df.drop(axis=0, index=0, inplace=True)        # drop init \n",
        "    (final_df.drop_duplicates(inplace = True))          # drop duplicates, if any\n",
        "    print(f'Length of final_df: {len(final_df)}')\n",
        "else:\n",
        "    ! wget \"https://www.dropbox.com/s/ioufzt7jx4qsv9s/all_evaluation_data.xlsx?dl=0\" -O \"all_evaluation_data.xlsx\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io6Ejog1J1OZ"
      },
      "source": [
        "def get_seprate_brands_df(df,labeled=False, transliterate=False, index2polarity={1: 'positive', -1: 'negative', 0: 'neutral'}):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        params:\n",
        "          df: output of fuzzy string matching code with text and brands list\n",
        "          labeled: save sentiment if True\n",
        "          transliterate: transliterate text and entity if True\n",
        "          index2polarity: dict with mapping of sentiment score to str\n",
        "        returns:\n",
        "          df with text, entity and sentiment(if input is labled) \n",
        "    \"\"\"\n",
        "    dict_list=[]\n",
        "    for index, row in df.iterrows():\n",
        "\n",
        "        entity_list = row[BRAND_LABEL_COL]\n",
        "        text = transliterator.transform(row[\"text\"]) if transliterate else row[\"text\"]\n",
        "        \n",
        "        if isinstance(entity_list, str):\n",
        "            entity_list = [w[0] for w in ast.literal_eval(entity_list)]\n",
        "        if labeled:\n",
        "            sentiment_list = [w.split(\"-\")[1] for w in row[SENTIMENT_LABEL_COL].split(\",\")]\n",
        "            for entity,sentiment in zip(entity_list,sentiment_list):\n",
        "                if sentiment==' ':\n",
        "                    sentiment=-1\n",
        "                dict_list.append({\n",
        "                    \"text\":text,\n",
        "                    \"entity\":entity,\n",
        "                    \"sentiment\":index2polarity[int(sentiment)]\n",
        "                })\n",
        "        else:\n",
        "            for entity in entity_list:\n",
        "                dict_list.append({\n",
        "                    \"text\":text,\n",
        "                    \"entity\":entity,\n",
        "                })\n",
        "                \n",
        "    return pd.DataFrame(dict_list)\n",
        "\n",
        "def is_english_using_spacy(strn):\n",
        "    \"\"\" returns True if text is english using spacy\"\"\"\n",
        "    doc = nlp(strn) \n",
        "    detect_language = doc._.language\n",
        "    return detect_language['language'] == 'en' and detect_language['score'] > 0.8\n",
        " \n",
        "def get_english_hindi_tweets_df_using_spacy(df, english=True):\n",
        "    \"\"\" returns df with english or non-english text\"\"\"\n",
        "    english_mask = [is_english_using_spacy(w) for w in df[\"text\"]]\n",
        "    return df[english_mask] if english else df[~np.array(english_mask)]\n",
        "\n",
        "def contains_non_ascii_char(strn):\n",
        "    for x in strn:\n",
        "        if ord(x) > 127:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def preprocess_df(df):\n",
        "    \"\"\" removes text containing non ascii charecters \"\"\"\n",
        "    mask = [False if ('′' in w or '″' in w or '…' in w or contains_non_ascii_char(w)) else True for w in df[\"text\"]]  # remove some punctuations and special unicode charecters\n",
        "    df = df[mask]\n",
        "    return df\n",
        "\n",
        "if not available:\n",
        "    BASE_DIR = \"\"\n",
        "    # df with text, entites and sentiment \n",
        "    df = pd.read_excel(os.path.join(BASE_DIR, \"all_evaluation_data.xlsx\"))        # input the train/test file\n",
        "    english_df = get_english_hindi_tweets_df_using_spacy(df, english=True)  # get english text\n",
        "    hindi_df = get_english_hindi_tweets_df_using_spacy(df, english=False)  # get hindi text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7YJXAJAJ3XM"
      },
      "source": [
        "if not available:\n",
        "    df = copy.deepcopy(english_df)\n",
        "    df = get_seprate_brands_df(df, labeled=False)  # get df text\n",
        "    (df.drop_duplicates(inplace = True))\n",
        "    df = df.rename(columns={\"text\":\"ori_text\"}, errors=\"raise\")\n",
        "\n",
        "    df2 = copy.deepcopy(final_df)\n",
        "    df2 = df2.rename(columns={0:\"text\"}, errors=\"raise\")\n",
        "\n",
        "    english_df_with_coref = pd.merge(df2, df, on=\"ori_text\")\n",
        "\n",
        "    df = copy.deepcopy(hindi_df)\n",
        "    df = get_seprate_brands_df(hindi_df, labeled=False, transliterate=True)\n",
        "    (df.drop_duplicates(inplace = True))\n",
        "    df[\"ori_text\"] = df[\"text\"]                                                     # obtain the df with Non English texts\n",
        "\n",
        "    df = pd.concat([df, english_df_with_coref],axis=0)                              # merge both df\n",
        "    df = preprocess_df(df)\n",
        "    df.to_csv(TEST_DF_FILE_PATH, index=False)                                       # this csv is being used by Kushal and Yash's model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O95cRdZjVn2O"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}